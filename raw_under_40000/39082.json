{"Nomor": 39082, "Judul": "ALOKASI SUMBER DAYA DINAMIS UNTUK DEEP LEARNING TRAINING MENGGUNAKAN TENSORFLOW PADA KLASTER KUBERNETES", "Abstrak": "Deep learning training terdistribusi pada saat ini menggunakan alokasi sumber\ndaya statis. Menggunakan arsitektur parameter server, training pada saat ini\nmemiliki jumlah ps node dan worker node yang tetap selama training berjalan\nhingga selesai. Bagi training yang berjalan pada klaster umum, yang sumber\ndayanya dibagi dengan proses-proses yang lain, adakalanya sumber daya menjadi\nbebas ketika training sedang berjalan, misalnya ketika proses lain yang\nmenggunakan sumber daya tersebut telah selesai. Dengan alokasi sumber daya\nstatis, training tidak dapat memanfaatkan sumber daya bebas ini. Pada tugas akhir\nini, dirancang dan diimplementasikan sistem alokasi sumber daya dinamis untuk\ntraining menggunakan TensorFlow yang berjalan pada klaster Kubernetes.\nImplementasi ini dilakukan dengan membangun sebuah komponen baru bernama\nConfigurationManager (CM). Komponen tersebut berperan untuk mengetahui\ninformasi sumber daya klaster serta menentukan penambahan ps node dan worker\nnode pada training. Selain itu, training diimplemetasikan agar dapat berkomunikasi\ndengan CM secara periodik. Hasil pengujian menunjukkan bahwa training dengan\nalokasi sumber daya dinamis lebih unggul daripada training dengan alokasi sumber\ndaya statis pada beberapa metrik kinerja, yakni penggunaan sumber daya klaster,\nwaktu per epoch, dan total waktu training. Namun demikian, training dengan\nalokasi sumber daya dinamis memiliki akurasi yang sedikit lebih rendah\ndibandingkan training dengan alokasi sumber daya statis.", "Daftar File": {"Abstrak": "https://digilib.itb.ac.id/gdl/download/170951"}, "Penulis": "Rahmad Yesa Surya [13515088]", "Kontributor / Dosen Pembimbing": ["Achmad Imam Kistijantoro, S.T., M.Sc., Ph.D."], "Jenis Koleksi": "Tugas Akhir", "Penerbit": "Teknik Informatika", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "TensorFlow, Kubernetes, distributed training, resource allocation", "Sumber": "", "Staf Input/Edit": "karya", "File": "8 file", "Tanggal Input": "21 Jun 2019"}