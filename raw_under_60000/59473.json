{"Nomor": 59473, "Judul": "RANCANG BANGUN ANIMASI WAJAH BERBASIS UCAPAN MENGGUNAKAN MODEL PEMBELAJARAN MESIN BERBASIS TRANSFORMER", "Abstrak": "Proses key-framing dalam produksi animasi membutuhkan waktu yang banyak\ndan merupakan pekerjaan yang melelahkan. Untuk animasi wajah, pembuatan\nanimasi dapat dibantu menggunakan performance capture berbasis visual yang\nmenangkap gerakan wajah aktor untuk secara otomatis menjadi keyframe pada\naplikasi animasi. Namun, solusi ini memerlukan perlengkapan yang mahal serta\npersiapan yang rumit. Pendekatan solusi lain adalah dengan membuat animasi\nwajah hanya dari suara ucapan sebagai alternatif dari pendekatan berbasis visual.\nPermasalahan utama untuk pendekatan ini adalah adanya fenomena coarticulation\npada ucapan manusia di mana konteks masa lalu dan konteks masa kini ucapan\nmempengaruhi representasi wajah masa kini. Saat ini penelitian-penelitian\npencarian metode terbaik masih dilakukan untuk menghasilkan pemetaan antara\nfitur suara dan fitur representasi wajah dengan akurasi yang tinggi serta kualitas\nanimasi yang baik.\nPada penelitian ini, implementasi dari model pembelajaran mesin berbasis\nTransformer diperkenalkan sebagai solusi alternatif dari solusi penelitianpenelitian sebelumnya. Mekanisme multi-head self-attention yang ada pada\nbagian encoder pada arsitektur Transformer digunakan untuk membangun sebuah\nmodel yang memprediksi fitur landmark wajah dari data sekuensial fitur ucapan.\nMekanisme attention memberikan penilaian fitur berdasarkan posisinya dalam\ndata sekuensial sehingga mekanisme ini dapat memodelkan fenomena\ncoarticulation dalam ucapan.\ni\nMel-Frequency Cepstral Coefficient (MFCC) berikut turunan pertama dan\nkeduanya dipilih sebagai fitur ucapan dan landmark wajah dipilih sebagai fitur\nrepresentasi wajah. Tidak ada fitur penengah yang digunakan seperti fonem dan\nsuku kata untuk mengejar solusi yang bebas dari keterikatan bahasa. Data\nlandmark wajah diperoleh dengan menggunakan OpenFace toolkit. Data yang\ndipilih untuk pembelajaran mesin bukanlah data landmark wajah langsung akan\ntetapi jarak posisi landmark dari landmark wajah diam untuk mendapatkan\npergerakan wajah yang bebas dari bentuk wajah pembicara. Data sekuensial baik\nmasukan dan keluaran untuk pembelajaran mesin dikontruksi menggunakan\nmetode sliding windows sehingga memiliki konteks masa lalu dan konteks masa\ndepan.\nUntuk mengukur kinerja hasil animasi yang dihasilkan, dilakukan kajian\nperbandingan dengan 3 solusi lain yaitu Multi Layer Perceptron (MLP),\nConvolutional Neural Network (CNN), dan Long Short-Term Memory (LSTM).\nHasil evaluasi akurasi prediksi menunjukkan bahwa metode Transformer\nmempunyai kinerja yang lebih baik dengan akurasi nilai Root Mean Squared\nError (RMSE) yaitu 0,9259 mm dibandingkan nilai RMSE dari keluaran MLP\nyaitu 0,9267 mm. Namun. kinerja akurasi RMSE Transformer masih di bawah\nakurasi LSTM yaitu 0,924 mm dan CNN yaitu 0,9244 mm.\nSelain akurasi, pada penelitian ini dilakukan pengujian waktu prediksi untuk\nsingle frame untuk melihat apakah sistem bisa digunakan pada aplikasi real time.\nHasil pengujian menunjukkan waktu prediksi rata-rata model Transformer adalah\n14,73 ms yang masih di bawah kriteria yaitu 16,66 ms apabila aplikasi\nmempunyai frame rate 60 frame per detik. Waktu prediksi ini tidak lebih baik dari\nwaktu prediksi model MLP, CNN, dan LSTM dengan masing-masing waktu\nprediksi 4,91 ms, 6,68 ms, dan 5,56 ms. Sehingga disimpulkan solusi-solusi lain\nlebih sesuai untuk aplikasi real time.\nKinerja hasil prediksi juga dievaluasi dari kehalusan kurva yang dihasilkan. Hasil\npengamatan kurva animasi dan spektrum frekuensi pada hasil-hasil prediksi\nii\nmenunjukkan bahwa terdapat noise yang menyebabkan ketidak halusan hasil\nanimasi MLP dan LSTM. Hal ini tidak ditemukan pada hasil model CNN dan\nTransformer. Pada penelitian ini juga dibuat algoritma penghalusan untuk\nkeluaran data sekuens yang dapat digunakan pada keluaran MLP dan\nTransformer. Berdasarkan pengamatan, algoritma ini dapat membuat hasil\nkeluaran Transformer mempunyai kehalusan animasi terbaik dibandingkan hasil\nkeluaran model lainnya walaupun pengujian menunjukkan tidak adanya\npeningkatan akurasi menggunakan RMSE. Berdasarkan hasil tersebut,\ndisimpulkan bahwa perlunya metrik tambahan atau metrik pengganti RMSE yang\nlebih representatif untuk mengukur kehalusan dari animasi yang dihasilkan.\nHasil prediksi divisualisasikan pada aplikasi Unity 3D untuk melihat lebih jauh\nhasil animasi yang dihasilkan. Pengamatan pada visualisasi animasi\nmemperlihatkan bahwa terdapat deformasi bentuk kepala ketika berbicara. Namun\nhal ini juga terlihat pada data asli keluaran OpenFace sehingga permasalahan\nberasal dari perolehan fitur data dari OpenFace. Untuk mengatasi masalah ini,\ndilakukan penskalaan nilai pergerakan yang berbeda-beda untuk setiap landmark.", "Daftar File": {}, "Penulis": "Mikhael Martin Nur Arief [23218107]", "Kontributor / Dosen Pembimbing": ["Dr. Reza Darmakusuma, S.T., M.T."], "Jenis Koleksi": "Tesis", "Penerbit": "Teknik Elektro", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "speech animation, lip-sync, Transformer", "Sumber": "", "Staf Input/Edit": "karya", "File": "0 file", "Tanggal Input": "10 Sep 2021"}