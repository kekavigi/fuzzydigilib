{"Nomor": 63359, "Judul": "PEMBELAJARAN TRANSFER DENGAN POST TRAINING UNTUK ANALISIS SENTIMEN BERBASIS ASPEK BERBAHASA INDONESIA", "Abstrak": "ABSTRAK\nPEMBELAJARAN TRANSFER DENGAN POST TRAINING\nUNTUK ANALISIS SENTIMEN BERBASIS ASPEK\nBERBAHASA INDONESIA\nOleh\nI Putu Eka Surya Aditya\nNIM: 23530053\n(Program Studi Magister Informatika)\nAnalisis sentimen berbasis aspek memiliki peran penting dalam pengembangan\nbisnis karena memudahkan pelaku bisnis untuk mengevaluasi umpan balik dari\npelanggan pada setiap aspek layanan. Dalam beberapa tahun terakhir, pre-trained\nmodel bahasa seperti ELMo, BERT, XLM-R dan XLNet telah mencapai sukses\nbesar dalam tugas pemrosesan bahasa alami (NLP) khususnya analisis sentimen\nberbasis aspek. Untuk Bahasa Indonesia sendiri telah ada beberapa penelitian\nmengenai tugas analisis sentimen berbasis aspek. Penelitian terbaru oleh (Azhar\ndan Khodra, 2020) menggunakan mBERT sebagai pre-trained model bahasa dan\nberhasil mencapai mencapai kinerja terbaik untuk data ulasan berdomain hotel.\nPendekatan yang digunakan oleh (Azhar dan Khodra, 2020) adalah penggunaan\nkalimat bantu yang diadaptasi dari penelitian (Sun dkk., 2019). Terdapat\npendekatan lain yang juga mencapai kinerja bagus pada tugas analisis sentimen\nberbasis aspek, yaitu post-training oleh (Xu dkk., 2019). Pada penelitiannya (Xu\ndkk., 2019) melakukan post-training pada untuk tugas analisis sentimen berbasis\naspek dan joint post-training pada tugas Review Reading Comprehension (RRC).\nPada penelitian ini dilakukan pengujian untuk melihat pengaruh pendekatan posttraining\ndan joint post-training pada task klasifikasi sentimen berbasis aspek\ndengan menggunakan pre-trained model bahasa yang berbeda dari penelitian\nsebelumnya.\nPada penelitian ini, digunakan tiga pre-trained model bahasa, yaitu: BERT\n(mBERT dan IndoBERT), XLM-R, dan XLNet (XLNet English dan XLNet\nMalay). Untuk pendekatan penyelesaian masalah digunakan dua pendekatan yaitu\npenggunaan kalimat bantu (Sun dkk., 2019) dan post-training/joint post-training\n(Xu dkk., 2019). Data yang digunakan pada penelitian ini dibagi menjadi tiga jenis\ndata, yaitu data untuk post-training, data untuk joint post-training dan data untuk\npelatihan dan pengujian. Data untuk post-training adalah ulasan hotel tanpa label\n(unsupervised), data untuk joint post-training adalah data ulasan mobil, dan data\nuntuk pelatihan dan pengujian sama dengan data yang digunakan pada penelitian\n(Azhar dan Khodra, 2020).\nHasil pengujian menunjukkan bahwa IndoBERT memiliki kinerja yang lebih baik\ndari model baseline (mBERT) baik dengan atau tanpa pendekatan post-training.\nPendekatan post-training pada XLM-R mencapai kinerja terbaik dengan F1-score\nsebesar 0.9875 pada data Uji 1 dan 0.9614 pada data Uji II. Model tersebut\nmengungguli baseline (mBERT tanpa post-training) sebesar 1.04% pada data Uji\nii\n1 dan 2.92% pada data uji 2. Hal ini dikarenakan XLM-R dilatih dengan parameter\nyang jauh lebih banyak dan ukuran kamus yang jauh lebih besar dari mBERT. Hasil\npengujian juga menunjukkan kinerja model hasil post-training mengungguli model\nhasil joint post-training pada semua pre-trained model bahasa. Model pada tesis ini\nmencapai kinerja terbaik pada data ulasan hotel berbahasa Indonesia (HoASA).\nKata kunci: analisis sentimen berbasis aspek, NLP, pre-trained model bahasa,\nIndoBERT, XLM-R, XLNet, kalimat bantu, post-training, joint post-training.", "Daftar File": {"ABSTRAK - I PUTU EKA SURYA ADITYA": "https://digilib.itb.ac.id/gdl/download/242672"}, "Penulis": "I Putu Eka Surya Aditya [23530053]", "Kontributor / Dosen Pembimbing": ["Dr. Masayu Leylia Khodra, S.T., M.T."], "Jenis Koleksi": "Tesis", "Penerbit": "Program Studi Magister Instrumentasi dan Kontrol", "Fakultas": "", "Subjek": "", "Kata Kunci": "analisis sentimen berbasis aspek, NLP, pre-trained model bahasa, IndoBERT, XLM-R, XLNet, kalimat bantu, post-training, joint post-training.", "Sumber": "", "Staf Input/Edit": "Didin Syafruddin Asa, S.Sos", "File": "1 file", "Tanggal Input": "02 Feb 2022"}