{"Nomor": 69200, "Judul": "MULTI-TASK LEARNING UNTUK NATURAL LANGUAGE UNDERSTANDING DALAM BAHASA INDONESIA DENGAN  ALGORITMA MACHAMP", "Abstrak": "Multi-task learning adalah pendekatan yang memanfaatkan pembelajaran beberapa task secara\nsekaligus untuk meningkatkan kinerja suatu model bahasa dalam menyelesaikan task-task\npemrosesan bahasa alami. Multi-task learning dapat membantu meningkatkan natural\nlanguage understanding suatu model, yaitu pemahaman model terhadap teks berbahasa alami\nsecara umum, karena sifatnya yang berorientasi pembelajaran beberapa task untuk\nmendapatkan representasi informasi secara umum dan minimalisasi overfit terhadap task\nspesifik tertentu.\nPenelitian natural language understanding berbahasa Indonesia sejauh ini berorientasi pada\nmodel single task, sehingga kinerja pendekatan tersebut dapat dibandingkan dengan\npendekatan multi-task. Salah satu model multi-task learning yang dapat dimanfaatkan untuk\npenelitian task-task natural language understanding berbahasa Indonesia adalah MACHAMP\n(Massive Choice, Ample Tasks). MACHAMP mengintegrasikan model pralatih sebagai\n\nencoder layer yang menyediakan shared representation untuk diproses oleh lapisan task-\nspecific. Selain itu, MACHAMP juga menyediakan konfigurasi untuk variasi task yang lebih\n\nberagam dan lebih mudah untuk dimodifikasi dibandingkan dengan framework-framework\nmulti-task learning sebelumnya sehingga dapat digunakan untuk melatih task-task NLU\nberbahasa Indonesia.\nPenelitian memanfaatkan 5 task klasifikasi teks dan 6 task sequence labelling untuk dilatih dan\ndiujikan dengan MACHAMP yang menggunakan IndoBERT sebagai encoder layer. Selain\nperbandingan dengan model single-task, penelitian juga menguji variasi kombinasi task yang\nberbeda dan panjang epoch pengujian. Hasil yang didapatkan adalah model multi-task dengan\nalgoritma MACHAMP yang diintegrasikan dengan IndoBERT memiliki rata-rata kinerja yang\nlebih baik yang signifikan secara statistik dibandingkan model single-task IndoBERT dengan\nrata-rata nilai F1-score 83.48 untuk semua task yang dilatih dibandingkan dengan 82.99 pada\nmodel IndoBERT. Hasil eksperimen juga menunjukkan bahwa model yang dilatih dengan\nkelompok task dengan similaritas lebih tinggi memiliki kinerja lebih baik.", "Daftar File": {}, "Penulis": "Faris Muhammad Kautsar [13518105]", "Kontributor / Dosen Pembimbing": ["Dr.Eng. Ayu Purwarianti, S.T., M.T."], "Jenis Koleksi": "Tugas Akhir", "Penerbit": "Teknik Informatika", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "multi-task learning, natural language understanding, Bahasa Indonesia, MACHAMP.", "Sumber": "", "Staf Input/Edit": "Dessy Rondang Monaomi", "File": "0 file", "Tanggal Input": "21 Sep 2022"}