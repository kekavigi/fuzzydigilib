{"Nomor": 66654, "Judul": "PENYELESAIAN COREFERENCE RESOLUTION BAHASA INDONESIA MENGGUNAKAN ARSITEKTUR WORD LEVEL  COREFERENCE RESOLUTION", "Abstrak": "Coreference resolution adalah suatu permasalahan pada bidang pemrosesan teks\nuntuk menemukan semua penyebutan (mention) yang merujuk kepada entitas yang\nsama di dunia nyata. Coreference resolution dapat digunakan untuk membantu\nmenyelesaikan permasalahan dalam pemrosesan bahasa alami yang lain yaitu entity\nlinking, machine translation, summarization, chatbots, dan question answering.\nPenelitian coreference resolution (coref) untuk Bahasa Indonesia masih minim.\nPenelitian coref pada Bahasa Indonesia relatif tidak dapat dibandingkan satu sama\nlain karena data yang dipakai relatif berbeda.\nPermasalahan yang terdapat pada coref Bahasa Indonesia yaitu permasalahan pada\ndataset dan permasalahan pada algoritma. Permasalahan pada dataset yaitu: tidak\nadanya dataset standar yang dapat dipakai sebagai benchmark. Permasalahan pada\nalgoritma yaitu belum adanya penelitian yang menggunakan metode terbaru dari\nbeberapa arsitektur deep learning yang meraih kinerja kompetitif sebagaimana\ndalam dataset Bahasa Inggris. Permasalahan algoritma yang lain yaitu penelitian\nterbaik sebelumnya masih menggunakan pendekatan pipelined system.\nTesis ini adalah bagian dari riset bersama yang dilakukan oleh ITB, Prosa.ai, dan\nAI Singapore. Riset yang dilakukan mencakup pembuatan Coreference Resolution\nin the Indonesian Language (COIN) dataset dengan standar yang disesuaikan\ndengan standar OntoNotes dataset dan pemodelan dengan menggunakan arsitektur\nc2f-coref dan wl-coref. Tesis ini memiliki lingkup untuk membangun kode program\ndan melaksanakan eksperimen dengan menggunakan arsitektur word level\ncoreference resolution (wl-coref). Selain itu, terdapat eksperimen arsitektur\nHigher-order Coreference Resolution with Coarse-to-fine Inference (c2f-coref)\ndengan variasi BERT encoder yang dikerjakan oleh engineer dari AI Singapore.\nAnalisis dilakukan bersama untuk membandingkan dan menganalisis performa\nmodel.\nArsitektur wl-coref dipilih sebagai solusi pada Tesis ini dikarenakan efisiensi dan\nkinerja yang kompetitif. Langkah pada arsitektur wl-coref adalah pencarian\ncoreference links antar token kata, kemudian melakukan konstruksi span dari token\n\nII\n\nyang memiliki coreference links. Proses adaptasi yang dilakukan pada arsitektur\nwl-coref mencakup perubahan pada pairwise feature (hand crafted feature) dengan\nhanya menggunakan jarak antar span dikarenakan pairwise feature lainnya tidak\ntersedia pada dataset COIN. Selain itu, arsitektur wl-coref membutuhkan data\ndependency relation untuk digunakan sebagai data pada modul span construction.\nSedangkan pada dataset COIN informasi tersebut tidak tersedia, sehingga data\ndependency relation tersebut dibangkitkan menggunakan library stanza.\nBerdasarkan hasil eksperimen, arsitektur wl-coref (F1 score 76.24) lebih baik\ndibandingkan dengan arsitektur c2f-coref (F1 score 76.02). Namun selisih kinerja\ndiantara keduannya tidak terlalu besar. Hal ini dapat disebabkan karena data\ndependency relation yang digunakan pada wl-coref Bahasa Indonesia dibangkitkan\nmenggunakan stanza, sedangkan pada Bahasa Inggris data tersebut dianotasi\nmanual. Sehingga hal tersebut dapat menimbulkan kesalahan lebih pada arsitektur\nwl-coref Bahasa Indonesia. Encoder terbaik untuk arsitektur wl-coref dan c2f-coref\n\npada Bahasa Indonesia adalah XLM-RoBERTa-large. Selain itu, IndoSpanBERT-\nlarge memberikan kinerja yang kompetitif dibawah XLM-RoBERTa-large,\n\nsehingga dapat menjadi pilihan encoder yang masih bagus dengan ukuran model\nyang lebih ringan. Pengujian pada metrik LEA menujukkan bahwa terdapat\nkecenderungan model yang bagus pada metrik CoNLL akan bagus pula pada metrik\nLEA. Walaupun LEA dan metrik CoNLL memiliki pendekatan perhitungan yang\nberbeda.\nBerdasarkan pengamatan mention recall pada beberapa variasi tipe mention dan\npanjang mention menunjukkan bahwa tipe mention yang memiliki banyak instances\ncenderung memiliki mention recall yang lebih bagus dibandingkan dengan mention\nrecall pada tipe mention yang memiliki sedikit instances. Selain itu, semakin\npanjang mention maka model cenderung mendapatkan mention recall yang lebih\nsedikit. Eksperimen hyperparameter tuning pada Tesis ini membuktikan bahwa\nhyperparameter default dari penelitian Dobrovolskii (2021) adalah hyperparameter\nterbaik.", "Daftar File": {}, "Penulis": "Fajar Muslim [23521044]", "Kontributor / Dosen Pembimbing": ["Dr.Eng. Ayu Purwarianti, S.T., M.T.", "Dr. Nur Ulfa Maulidevi, S.T., M.Sc.", "Fariska Zakhralativa Ruskanda, S.T., M.T."], "Jenis Koleksi": "Tesis", "Penerbit": "Informatika", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "coreference resolution, dataset, arsitektur word-level, XLM-RoBERTa, IndoSpanBERT, mention recall.", "Sumber": "", "Staf Input/Edit": "Dessy Rondang Monaomi", "File": "0 file", "Tanggal Input": "29 Jun 2022"}