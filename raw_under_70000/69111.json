{"Nomor": 69111, "Judul": "IMPLEMENTASI KUANTISASI TERHADAP MODEL BERT BAHASA  INDONESIA", "Abstrak": "Beberapa tahun belakangan ini, pemanfaatan model pra-latih telah mendominasi penelitian\nkomputasi di berbagai bidang, termasuk pemrosesan bahasa alami. Salah satu model pra-latih yang\npopuler adalah Bidirectional Encoder Representations from Transformers (BERT). BERT berhasil\nmenjadi state-of-the-art diantara model-model yang lain dan diadaptasi ke berbagai bahasa,\ntermasuk bahasa Indonesia, IndoBERT. Sebagaimana model BERT, IndoBERT memiliki ukuran\nyang besar sehingga menimbulkan isu terkait latency dan efisiensi dari modelnya. Untuk\nmengatasi masalah efisisiensi di IndoBERT, dalam studi ini kami mengeksplorasi kemungkinan\npenggunaan kuantisasi untuk mengkompresi IndoBERT.\nKuantisasi adalah sebuah teknik untuk melakukan komputasi dan penyimpanan tensor pada\npresisi bit yang lebih kecil. Kuantisasi memiliki kelebihan yakni kuantisasi hanya mengubah\nukuran bit pada bobot model, sehingga arsitektur model tidak perlu diubah dan desain model yang\nlebih kecil juga tidak diperlukan. Kuantisasi juga memiliki penurunan performa yang cukup rendah\nhingga tanpa pengurangan sama sekali. Metode kuantisasi yang populer adalah post training\nquantization dan quantization aware training. Post training quantization merupakan metode\nkuantisasi dimana bobot dari model yang telah di fine tuning dikurangi presisi bitnya. Sedangkan\nQuantization aware training merupakan metode dimana operasi kuantisasi dalam model\ndimasukkan pada saat training/fine tuning dengan tujuan untuk membuat model dapat beradaptasi\ndengan bobot dan aktivasi yang dikuantisasi.\nEksperimen dilakukan dengan 7 downstream task dan didapatkan hasil bahwa model\nmemiliki kinerja yang baik jika dibandingkan dengan model full precision. Terdapat penurunan\nkinerja pada kasus ekstrem seperti kuantisasi 4 bit. Downstream task berjenis sequence labeling\njuga terbukti memiliki sensitivitas yang lebih tinggi. Hasil eksperimen juga menunjukkan bahwa\npenurunan performa dapat diminimalisir dengan metode Quantization Aware Training.", "Daftar File": {}, "Penulis": "Muhammad Ayyub Abdurrahman [13518076]", "Kontributor / Dosen Pembimbing": ["Dr.Eng. Ayu Purwarianti, S.T., M.T."], "Jenis Koleksi": "Tugas Akhir", "Penerbit": "Teknik Informatika", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "kuantisasi, model BERT, kompresi model", "Sumber": "", "Staf Input/Edit": "Dessy Rondang Monaomi", "File": "0 file", "Tanggal Input": "20 Sep 2022"}