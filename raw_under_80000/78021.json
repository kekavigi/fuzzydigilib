{"Nomor": 78021, "Judul": "EXTRACTIVE SUMMARIZATION DENGAN TEXT ENCODER SENTENCE-BERT DAN REINFORCEMENT LEARNING UNTUK TEKS BAHASA INDONESIA", "Abstrak": "Dalam era perkembangan teknologi yang pesat, informasi yang melimpah dari internet dapat menyebabkan information overload. Untuk mengatasi masalah ini, domain penelitian text summarization dikembangkan dengan tujuan mengekstrak intisari dari suatu dokumen. Saat ini, pendekatan neural network dalam text summarization bertipe ekstraktif dengan teknik deep learning telah mengungguli pendekatan lain. Namun, terdapat masalah diskrepansi antara objektif pelatihan model yaitu cross entropy loss dengan metriks evaluasi model, yaitu ROUGE. Untuk mengatasi perbedaan ini, beberapa penelitian menggunakan pendekatan reinforcement learning. Contohnya adalah REFRESH. REFRESH secara langsung menggunakan ROUGE sebagai salah satu komponen dalam fungsi objektif dalam pelatihan model. Namun, dalam implementasi REFRESH, terdapat permasalahan long-distance dependency yang disebabkan oleh penggunaan CNN sebagai sentence encoder. Maka dari itu, penelitian ini mengusulkan pemanfaatan SentenceBERT (SBERT) sebagai sentence encoder alternatif pengganti CNN dalam REFRESH. SBERT adalah sebuah model berbasis transformer yang mampu menghasilkan representasi vektor sebuah kalimat yang bermakna secara semantik dan mampu mengatasi permasalahan long-distance dependency. Namun, saat ini belum ada SBERT untuk Bahasa Indonesia.\n\nPenelitian ini menyajikan dua kontribusi utama. Pertama, dikembangkan model\nsentence embedding khusus untuk Bahasa Indonesia, yang disebut IndoSBERT,\n \nyang dilatih dengan menggunakan arsitektur jaringan siamese network terhadap tugas Semantic Textual Similarity yang mampu menghasilkan representasi kalimat Bahasa Indonesia yang bermakna secara semantik. Model ini akan digunakan dalam REFRESH sebagai pengganti CNN untuk bagian sentence encoder, dalam rangka menghindari masalah long distance dependency. Kedua, dikembangkan REFRESH untuk Bahasa Indonesia dengan memanfaatkan IndoSBERT yang sudah dibuat sebelumnya.\n\nModel IndoSBERT menunjukkan peningkatan kinerja dalam tugas Semantic Textual Similarity dibandingkan dengan model IndoBERT dan beberapa model multilingual lainnya, dengan nilai Spearman Rank Correlation Score 0.856. Dalam pengujian model REFRESH, penggunaan IndoSBERT sebagai sentence embedding juga menghasilkan skor ROUGE yang lebih tinggi dibandingkan dengan penggunaan CNN sebagai sentence embedding. IndoSBERT-REFRESH menghasilkan nilai ROUGE-1 0.324, lebih baik dari CNN-REFRESH dengan nilai ROUGE-1 0.273.", "Daftar File": {"Kadek Denaya Rahadika Diana [23521027].pdf": "https://digilib.itb.ac.id/gdl/download/309418"}, "Penulis": "Kadek Denaya Rahadika Diana [23521027]", "Kontributor / Dosen Pembimbing": ["Dr. Masayu Leylia Khodra, S.T, M.T."], "Jenis Koleksi": "Tesis", "Penerbit": "Informatika", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "IndoSBERT, peringkasan teks ekstraktif, REFRESH, reinforcement learning, sentence embedding.", "Sumber": "", "Staf Input/Edit": "Dessy Rondang Monaomi \u00a0\n                                     Dessy Rondang Monaomi", "File": "1 file", "Tanggal Input": "15 Sep 2023"}