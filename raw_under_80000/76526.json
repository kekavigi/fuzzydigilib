{"Nomor": 76526, "Judul": "PEMBANGUNAN METODE LANGUAGEFUSION UNTUK MENGATASI LANGUAGE IDENTIFICATION BOTTLENECK PADA MODULAR MULTILINGUAL LANGUAGE MODELS DALAM INTENT  CLASSIFICATION TASK", "Abstrak": "Perkembangan LLM (Large Language Models) yang pesat telah menimbulkan\ntantangan ketika \u201clanguage identification bottleneck\u201d menjadi salah satu masalah\nyang penting pada konteks multilingual. Fenomena ini berdampak signifikan\nterhadap efektivitas intent classification dalam task-oriented dialogue system\ndalam skenario multilingual. Penelitian ini menyelidiki pengaruh modul\nlanguage identification yang \u201coff-the-shelf \u201d (LID) terhadap kinerja model dan\nmembandingkannya dalam skenario di mana bahasa input sudah diketahui dan\ndibandingkan ketika bahasa input tidak diketahui. Dari eksperimen yang dilafkukan,\npenggunaan LID masih belum cukup baik untuk bisa mengungguli kinerja model\nyang menggunakan teknik fine-tuning secara multilingual.\nPenelitian ini mengusulkan sebuah metode baru bernama LanguageFusion yang\nmemungkinkan modular LM melakukan inferensi pada input bahasa yang tidak\ndiketahui tanpa perlu menggunakan modul LID eksternal. Penelitian ini mengusulkan\nmetode yang memungkinkan model bahasa modular untuk menangani situasi ketika\nbahasa input tidak diketahui, menyediakan solusi praktis untuk aplikasi multilingual\ndari LLM. LanguageFusion bekerja dengan cara memanfaatkan suatu fusion layer\nyang berguna untuk menentukan attention weights dari masing-masing modul\nadapter yang digunakan.\nSkenario baseline eksperimen yang dijalankan yaitu ketika ada suatu input kalimat\ndalam suatu bahasa, LID akan melakukan deteksi bahasa terhadap kalimat input ini.\nSetelah ditentukan bahasa dari kalimat input, sebuah pretrained language adapter\nakan dipasangkan terhadap suatu LLM. Kemudian, dilakukan proses inferensi\nterhadap teks input oleh LLM + Adapter. Metode ini kemudian juga dibandingkan\ndengan metode LanguageFusion yang diusulkan, dan didapatkan hasil bahwa metode\nLanguageFusion mampu mengungguli model mBERT-vanilla dan MAD-X dalam\nmenangani language identification bottleneck, terutama dalam hasil pengujian unseen\nlanguages dengan margin sebesar 2.47% dibandingkan dengan metode lain.", "Daftar File": {"23522045 Muhammad Farid Adilazuarda.pdf": "https://digilib.itb.ac.id/gdl/download/303533"}, "Penulis": "Muhammad Farid Adilazuarda [23522045]", "Kontributor / Dosen Pembimbing": ["Dr. Eng. Ayu Purwarianti, S.T, M.T."], "Jenis Koleksi": "Tesis", "Penerbit": "Informatika", "Fakultas": "Sekolah Teknik Elektro dan Informatika", "Subjek": "", "Kata Kunci": "LanguageFusion, Pretrained Language Models, Language Identification Bottleneck, Parameter-Efficient Fine-Tuning, Adapter", "Sumber": "", "Staf Input/Edit": "Dessy Rondang Monaomi", "File": "1 file", "Tanggal Input": "16 Agu 2023"}